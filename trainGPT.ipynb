{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"trainGPT.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1_QGoUOQt3ml4IF1nefw82OuMJkzvwcUq","authorship_tag":"ABX9TyMckNSC9DFKhNPqHwwy+mGK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"oD54Na265Ogx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633397629195,"user_tz":-540,"elapsed":635,"user":{"displayName":"김민찬","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijYNt1jFISbeAVa495-l-MCVOZ78NvAChvOHH1Bw=s64","userId":"02581483532226309311"}},"outputId":"2b2919f5-8938-4b0e-a232-7e57b4d3a1fb"},"source":["!CUDA_LAUNCH_BLOCKING=1\n","!nvidia-smi\n"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Oct  5 01:33:48 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   51C    P0    32W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"id":"byRi_fSm5P-6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633397633170,"user_tz":-540,"elapsed":3554,"user":{"displayName":"김민찬","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijYNt1jFISbeAVa495-l-MCVOZ78NvAChvOHH1Bw=s64","userId":"02581483532226309311"}},"outputId":"705270d4-4a23-48ec-e2b4-656486038cad"},"source":["!pip install -r /content/drive/MyDrive/textG/requirements.txt"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gluonnlp>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/textG/requirements.txt (line 1)) (0.10.0)\n","Requirement already satisfied: mxnet in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/textG/requirements.txt (line 2)) (1.8.0.post0)\n","Requirement already satisfied: sentencepiece>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/textG/requirements.txt (line 3)) (0.1.96)\n","Requirement already satisfied: torch==1.5.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/textG/requirements.txt (line 4)) (1.5.0)\n","Requirement already satisfied: transformers>=2.1.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/textG/requirements.txt (line 5)) (4.11.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/textG/requirements.txt (line 6)) (4.62.3)\n","Requirement already satisfied: kss in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/textG/requirements.txt (line 7)) (3.2.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.5.0->-r /content/drive/MyDrive/textG/requirements.txt (line 4)) (1.19.5)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.5.0->-r /content/drive/MyDrive/textG/requirements.txt (line 4)) (0.16.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp>=0.8.3->-r /content/drive/MyDrive/textG/requirements.txt (line 1)) (21.0)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp>=0.8.3->-r /content/drive/MyDrive/textG/requirements.txt (line 1)) (0.29.24)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=2.1.1->-r /content/drive/MyDrive/textG/requirements.txt (line 5)) (3.0.12)\n","Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.1.1->-r /content/drive/MyDrive/textG/requirements.txt (line 5)) (0.0.18)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.1.1->-r /content/drive/MyDrive/textG/requirements.txt (line 5)) (5.4.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=2.1.1->-r /content/drive/MyDrive/textG/requirements.txt (line 5)) (0.0.46)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=2.1.1->-r /content/drive/MyDrive/textG/requirements.txt (line 5)) (2.23.0)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.1.1->-r /content/drive/MyDrive/textG/requirements.txt (line 5)) (0.10.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.1.1->-r /content/drive/MyDrive/textG/requirements.txt (line 5)) (2019.12.20)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=2.1.1->-r /content/drive/MyDrive/textG/requirements.txt (line 5)) (4.8.1)\n","Requirement already satisfied: ruamel.yaml==0.17.16 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers>=2.1.1->-r /content/drive/MyDrive/textG/requirements.txt (line 5)) (0.17.16)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers>=2.1.1->-r /content/drive/MyDrive/textG/requirements.txt (line 5)) (3.7.4.3)\n","Requirement already satisfied: ruamel.yaml.clib>=0.1.2 in /usr/local/lib/python3.7/dist-packages (from ruamel.yaml==0.17.16->huggingface-hub>=0.0.17->transformers>=2.1.1->-r /content/drive/MyDrive/textG/requirements.txt (line 5)) (0.2.6)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp>=0.8.3->-r /content/drive/MyDrive/textG/requirements.txt (line 1)) (2.4.7)\n","Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from mxnet->-r /content/drive/MyDrive/textG/requirements.txt (line 2)) (0.8.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.1.1->-r /content/drive/MyDrive/textG/requirements.txt (line 5)) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.1.1->-r /content/drive/MyDrive/textG/requirements.txt (line 5)) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.1.1->-r /content/drive/MyDrive/textG/requirements.txt (line 5)) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.1.1->-r /content/drive/MyDrive/textG/requirements.txt (line 5)) (2.10)\n","Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (from kss->-r /content/drive/MyDrive/textG/requirements.txt (line 7)) (1.5.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=2.1.1->-r /content/drive/MyDrive/textG/requirements.txt (line 5)) (3.5.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=2.1.1->-r /content/drive/MyDrive/textG/requirements.txt (line 5)) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=2.1.1->-r /content/drive/MyDrive/textG/requirements.txt (line 5)) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=2.1.1->-r /content/drive/MyDrive/textG/requirements.txt (line 5)) (7.1.2)\n"]}]},{"cell_type":"code","metadata":{"id":"qBKuncy_5u5d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633397633171,"user_tz":-540,"elapsed":8,"user":{"displayName":"김민찬","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijYNt1jFISbeAVa495-l-MCVOZ78NvAChvOHH1Bw=s64","userId":"02581483532226309311"}},"outputId":"98130e07-eda7-4324-b886-b2844f7bff22"},"source":["import os\n","import sys\n","sys.path.append('/content/drive/MyDrive/textG')\n","print(os.getcwd())\n","\n"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","metadata":{"id":"ZAJkJrHg6kQ9","executionInfo":{"status":"ok","timestamp":1633397637225,"user_tz":-540,"elapsed":4057,"user":{"displayName":"김민찬","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijYNt1jFISbeAVa495-l-MCVOZ78NvAChvOHH1Bw=s64","userId":"02581483532226309311"}}},"source":["import random\n","import torch\n","from torch.utils.data import DataLoader # 데이터로더\n","from gluonnlp.data import SentencepieceTokenizer \n","from model.torch_gpt2 import GPT2Config, GPT2LMHeadModel # model폴더의 torch_gpt2.py의 \n","# from transformers.configuration_gpt2 import GPT2Config 를 from transformers import GPT2Config로 변경해 오류 잡음\n","from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n","from util.data import FairyDataset\n","import gluonnlp\n","from tqdm import tqdm\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"SfIgiobk7ITZ","executionInfo":{"status":"ok","timestamp":1633397637226,"user_tz":-540,"elapsed":31,"user":{"displayName":"김민찬","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijYNt1jFISbeAVa495-l-MCVOZ78NvAChvOHH1Bw=s64","userId":"02581483532226309311"}}},"source":["ctx= 'cuda'#'cuda' #'cpu' #학습 Device CPU or GPU. colab의 경우 GPU 사용\n","cachedir='/content/drive/MyDrive/textG/kogpt2/' # KoGPT-2 모델 다운로드 경로\n","save_path = '/content/drive/MyDrive/textG/checkpoint/'\n","use_cuda = True # Colab내 GPU 사용을 위한 값\n","\n","kogpt2_config = {\n","    \"initializer_range\": 0.02,\n","    \"layer_norm_epsilon\": 1e-05,\n","    \"n_ctx\": 1024,\n","    \"n_embd\": 768,\n","    \"n_head\": 12,\n","    \"n_layer\": 12,\n","    \"n_positions\": 1024,\n","    \"vocab_size\": 51200\n","}"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X8qrmEdc8t-A","executionInfo":{"status":"ok","timestamp":1633397651414,"user_tz":-540,"elapsed":14219,"user":{"displayName":"김민찬","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijYNt1jFISbeAVa495-l-MCVOZ78NvAChvOHH1Bw=s64","userId":"02581483532226309311"}},"outputId":"8170e253-4dc1-4f9c-f7d6-794b5b219293"},"source":["# KoGPT-2 언어 모델 학습을 위한 GPT2LMHeadModel 선언\n","kogpt2model = GPT2LMHeadModel(config=GPT2Config.from_dict(kogpt2_config))\n","# model_path로부터 다운로드 받은 내용을 load_state_dict으로 업로드\n","kogpt2model.from_pretrained(\"skt/kogpt2-base-v2\")\n","\n","# 추가로 학습하기 위해 .train() 사용\n","kogpt2model.train()\n","vocab_b_obj = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\", bos_token='<s>', eos_token='</s>', unk_token='<unk>',  pad_token='<pad>', mask_token='<mask>') \n"," \n"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n","The class this function is called from is 'PreTrainedTokenizerFast'.\n"]}]},{"cell_type":"code","metadata":{"id":"jwYyYKAn8-9b"},"source":["model, vocab = kogpt2model, vocab_b_obj.get_vocab()\n","sentencepieceTokenizer = vocab_b_obj.tokenize\n","#os.chdir(\"../\")\n","# data_file_path = '/content/drive/MyDrive/Colab Notebooks/narrativeKoGPT2/data/backmyo_novel_1/dataset_0825.txt'\n","data_file_path = '/content/drive/MyDrive/Colab Notebooks/create_data/use_data/구텐베르크 동화/total_data.txt'\n","# /content/drive/MyDrive/Colab Notebooks/create_data/use_data/dataset_2.txt 이솝우화 약 312개\n","dataset = FairyDataset(data_file_path, vocab, sentencepieceTokenizer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t77J70lx9LI6","executionInfo":{"status":"ok","timestamp":1633397724183,"user_tz":-540,"elapsed":2,"user":{"displayName":"김민찬","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijYNt1jFISbeAVa495-l-MCVOZ78NvAChvOHH1Bw=s64","userId":"02581483532226309311"}}},"source":["batch_size = 4 # batch_size 1이면 오류안남 2면 오류남/ 고침\n","fairy_data_loader = DataLoader(dataset, batch_size=batch_size)"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"3AZVRuy-9Y9Y","executionInfo":{"status":"ok","timestamp":1633397724704,"user_tz":-540,"elapsed":2,"user":{"displayName":"김민찬","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijYNt1jFISbeAVa495-l-MCVOZ78NvAChvOHH1Bw=s64","userId":"02581483532226309311"}}},"source":["# 1e-4 5e-5 2.5e-5 2e-5\n","learning_rate = 1e-4 # 학습률 잠시 수정 원래는 1e-5임\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"xVGRKnH_AQgD","executionInfo":{"status":"ok","timestamp":1633397725138,"user_tz":-540,"elapsed":1,"user":{"displayName":"김민찬","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijYNt1jFISbeAVa495-l-MCVOZ78NvAChvOHH1Bw=s64","userId":"02581483532226309311"}}},"source":["# 메모리 오류 나면 사용\n","import torch, gc\n","gc.collect()\n","torch.cuda.empty_cache()"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FGawuzL_9ewu","executionInfo":{"status":"ok","timestamp":1633401220744,"user_tz":-540,"elapsed":3494976,"user":{"displayName":"김민찬","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GijYNt1jFISbeAVa495-l-MCVOZ78NvAChvOHH1Bw=s64","userId":"02581483532226309311"}},"outputId":"53a2ec44-303c-463e-e8fc-9f051d6b1cf5"},"source":["model.cuda()\n","print('KoGPT-2 Transfer Learning Start')\n","epochs=200\n","# 학습률에 따른 에폭 조정해서 돌아가도록 코드 수정해야함\n","for epoch in range(epochs):\n","    count = 0\n","    print(epoch)\n","    for data in fairy_data_loader:\n","        optimizer.zero_grad()\n","        data = torch.stack(data) # list of Tensor로 구성되어 있기 때문에 list를 stack을 통해 변환해준다.\n","        data= data.transpose(1,0)\n","        \n","        data= data.to(ctx)\n","   \n","        outputs = model(data, labels=data)\n","        loss, logits = outputs[:2]\n","        loss.backward()\n","        optimizer.step()\n","        if count %10 ==0:\n","            print('epoch no.{} train no.{}  loss = {}' . format(epoch, count+1, loss))\n","            # torch.save(model,save_path+'checkpoint_{}_{}.tar'.format(epoch,count))\n","            # 추론 및 학습 재개를 위한 일반 체크포인트 저장하기\n","\n","        count += 1\n","\n","print(\"save!\")\n","save_path = '/content/drive/MyDrive/Colab Notebooks/narrativeKoGPT2/checkpoint/'       \n","torch.save({\n","        'epoch': epoch,\n","        'train_no': count,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'loss':loss\n","    }, save_path+'gt_checkpoint_2.tar')"],"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["KoGPT-2 Transfer Learning Start\n","0\n","epoch no.0 train no.1  loss = 7.76411247253418\n","epoch no.0 train no.11  loss = 8.240137100219727\n","epoch no.0 train no.21  loss = 8.099803924560547\n","epoch no.0 train no.31  loss = 8.464111328125\n","1\n","epoch no.1 train no.1  loss = 7.237978935241699\n","epoch no.1 train no.11  loss = 7.607227325439453\n","epoch no.1 train no.21  loss = 7.618948459625244\n","epoch no.1 train no.31  loss = 8.079451560974121\n","2\n","epoch no.2 train no.1  loss = 7.025771617889404\n","epoch no.2 train no.11  loss = 7.274022102355957\n","epoch no.2 train no.21  loss = 7.264321804046631\n","epoch no.2 train no.31  loss = 7.755885601043701\n","3\n","epoch no.3 train no.1  loss = 6.672304630279541\n","epoch no.3 train no.11  loss = 6.976747512817383\n","epoch no.3 train no.21  loss = 6.92443323135376\n","epoch no.3 train no.31  loss = 7.436278343200684\n","4\n","epoch no.4 train no.1  loss = 6.395531177520752\n","epoch no.4 train no.11  loss = 6.638428211212158\n","epoch no.4 train no.21  loss = 6.588153839111328\n","epoch no.4 train no.31  loss = 7.145244121551514\n","5\n","epoch no.5 train no.1  loss = 6.1554646492004395\n","epoch no.5 train no.11  loss = 6.321449279785156\n","epoch no.5 train no.21  loss = 6.247837543487549\n","epoch no.5 train no.31  loss = 6.866549491882324\n","6\n","epoch no.6 train no.1  loss = 5.874508380889893\n","epoch no.6 train no.11  loss = 5.976057529449463\n","epoch no.6 train no.21  loss = 5.960721492767334\n","epoch no.6 train no.31  loss = 6.541949272155762\n","7\n","epoch no.7 train no.1  loss = 5.580996513366699\n","epoch no.7 train no.11  loss = 5.685678005218506\n","epoch no.7 train no.21  loss = 5.69612979888916\n","epoch no.7 train no.31  loss = 6.243576526641846\n","8\n","epoch no.8 train no.1  loss = 5.306248664855957\n","epoch no.8 train no.11  loss = 5.4209089279174805\n","epoch no.8 train no.21  loss = 5.402669906616211\n","epoch no.8 train no.31  loss = 5.981645584106445\n","9\n","epoch no.9 train no.1  loss = 5.0828657150268555\n","epoch no.9 train no.11  loss = 5.164917469024658\n","epoch no.9 train no.21  loss = 5.1544413566589355\n","epoch no.9 train no.31  loss = 5.717757225036621\n","10\n","epoch no.10 train no.1  loss = 4.864197731018066\n","epoch no.10 train no.11  loss = 4.923588752746582\n","epoch no.10 train no.21  loss = 4.914097785949707\n","epoch no.10 train no.31  loss = 5.471415996551514\n","11\n","epoch no.11 train no.1  loss = 4.634622573852539\n","epoch no.11 train no.11  loss = 4.687944412231445\n","epoch no.11 train no.21  loss = 4.668597221374512\n","epoch no.11 train no.31  loss = 5.200610160827637\n","12\n","epoch no.12 train no.1  loss = 4.433900356292725\n","epoch no.12 train no.11  loss = 4.465203285217285\n","epoch no.12 train no.21  loss = 4.453845500946045\n","epoch no.12 train no.31  loss = 4.972381591796875\n","13\n","epoch no.13 train no.1  loss = 4.208143711090088\n","epoch no.13 train no.11  loss = 4.239630222320557\n","epoch no.13 train no.21  loss = 4.235125541687012\n","epoch no.13 train no.31  loss = 4.737892150878906\n","14\n","epoch no.14 train no.1  loss = 4.013580322265625\n","epoch no.14 train no.11  loss = 4.025269985198975\n","epoch no.14 train no.21  loss = 4.039035797119141\n","epoch no.14 train no.31  loss = 4.514986991882324\n","15\n","epoch no.15 train no.1  loss = 3.8200478553771973\n","epoch no.15 train no.11  loss = 3.838636875152588\n","epoch no.15 train no.21  loss = 3.7916457653045654\n","epoch no.15 train no.31  loss = 4.286780834197998\n","16\n","epoch no.16 train no.1  loss = 3.66489577293396\n","epoch no.16 train no.11  loss = 3.650200843811035\n","epoch no.16 train no.21  loss = 3.6158034801483154\n","epoch no.16 train no.31  loss = 4.030834197998047\n","17\n","epoch no.17 train no.1  loss = 3.398434638977051\n","epoch no.17 train no.11  loss = 3.3993611335754395\n","epoch no.17 train no.21  loss = 3.391475200653076\n","epoch no.17 train no.31  loss = 3.776150941848755\n","18\n","epoch no.18 train no.1  loss = 3.1988277435302734\n","epoch no.18 train no.11  loss = 3.2035765647888184\n","epoch no.18 train no.21  loss = 3.168466806411743\n","epoch no.18 train no.31  loss = 3.5541932582855225\n","19\n","epoch no.19 train no.1  loss = 3.0073907375335693\n","epoch no.19 train no.11  loss = 2.995471954345703\n","epoch no.19 train no.21  loss = 2.9540865421295166\n","epoch no.19 train no.31  loss = 3.3527472019195557\n","20\n","epoch no.20 train no.1  loss = 2.822232246398926\n","epoch no.20 train no.11  loss = 2.7900798320770264\n","epoch no.20 train no.21  loss = 2.7404592037200928\n","epoch no.20 train no.31  loss = 3.0951662063598633\n","21\n","epoch no.21 train no.1  loss = 2.613476037979126\n","epoch no.21 train no.11  loss = 2.5958826541900635\n","epoch no.21 train no.21  loss = 2.529306411743164\n","epoch no.21 train no.31  loss = 2.9118518829345703\n","22\n","epoch no.22 train no.1  loss = 2.4571821689605713\n","epoch no.22 train no.11  loss = 2.409834623336792\n","epoch no.22 train no.21  loss = 2.3409664630889893\n","epoch no.22 train no.31  loss = 2.729904890060425\n","23\n","epoch no.23 train no.1  loss = 2.275308609008789\n","epoch no.23 train no.11  loss = 2.2414112091064453\n","epoch no.23 train no.21  loss = 2.1442630290985107\n","epoch no.23 train no.31  loss = 2.4818215370178223\n","24\n","epoch no.24 train no.1  loss = 2.069246768951416\n","epoch no.24 train no.11  loss = 2.046827793121338\n","epoch no.24 train no.21  loss = 1.9655156135559082\n","epoch no.24 train no.31  loss = 2.2587578296661377\n","25\n","epoch no.25 train no.1  loss = 1.9056419134140015\n","epoch no.25 train no.11  loss = 1.8844190835952759\n","epoch no.25 train no.21  loss = 1.8147460222244263\n","epoch no.25 train no.31  loss = 2.079493761062622\n","26\n","epoch no.26 train no.1  loss = 1.7374509572982788\n","epoch no.26 train no.11  loss = 1.7066696882247925\n","epoch no.26 train no.21  loss = 1.6252743005752563\n","epoch no.26 train no.31  loss = 1.9114261865615845\n","27\n","epoch no.27 train no.1  loss = 1.5607237815856934\n","epoch no.27 train no.11  loss = 1.5356569290161133\n","epoch no.27 train no.21  loss = 1.4899119138717651\n","epoch no.27 train no.31  loss = 1.7139631509780884\n","28\n","epoch no.28 train no.1  loss = 1.42235267162323\n","epoch no.28 train no.11  loss = 1.4133275747299194\n","epoch no.28 train no.21  loss = 1.321536660194397\n","epoch no.28 train no.31  loss = 1.5585609674453735\n","29\n","epoch no.29 train no.1  loss = 1.2645901441574097\n","epoch no.29 train no.11  loss = 1.252954125404358\n","epoch no.29 train no.21  loss = 1.1947643756866455\n","epoch no.29 train no.31  loss = 1.3562450408935547\n","30\n","epoch no.30 train no.1  loss = 1.1315778493881226\n","epoch no.30 train no.11  loss = 1.1146079301834106\n","epoch no.30 train no.21  loss = 1.040151596069336\n","epoch no.30 train no.31  loss = 1.238371729850769\n","31\n","epoch no.31 train no.1  loss = 1.010083556175232\n","epoch no.31 train no.11  loss = 1.000220537185669\n","epoch no.31 train no.21  loss = 0.9124343395233154\n","epoch no.31 train no.31  loss = 1.0732382535934448\n","32\n","epoch no.32 train no.1  loss = 0.8842156529426575\n","epoch no.32 train no.11  loss = 0.8698384165763855\n","epoch no.32 train no.21  loss = 0.8155151009559631\n","epoch no.32 train no.31  loss = 0.9169535040855408\n","33\n","epoch no.33 train no.1  loss = 0.8099104166030884\n","epoch no.33 train no.11  loss = 0.7941599488258362\n","epoch no.33 train no.21  loss = 0.7333419919013977\n","epoch no.33 train no.31  loss = 0.812457799911499\n","34\n","epoch no.34 train no.1  loss = 0.6999949812889099\n","epoch no.34 train no.11  loss = 0.7013611793518066\n","epoch no.34 train no.21  loss = 0.6391068696975708\n","epoch no.34 train no.31  loss = 0.7187036275863647\n","35\n","epoch no.35 train no.1  loss = 0.6032381057739258\n","epoch no.35 train no.11  loss = 0.6074861288070679\n","epoch no.35 train no.21  loss = 0.538749635219574\n","epoch no.35 train no.31  loss = 0.616804301738739\n","36\n","epoch no.36 train no.1  loss = 0.5167680382728577\n","epoch no.36 train no.11  loss = 0.5398885011672974\n","epoch no.36 train no.21  loss = 0.4700019955635071\n","epoch no.36 train no.31  loss = 0.5348635315895081\n","37\n","epoch no.37 train no.1  loss = 0.4639691710472107\n","epoch no.37 train no.11  loss = 0.45110684633255005\n","epoch no.37 train no.21  loss = 0.41600045561790466\n","epoch no.37 train no.31  loss = 0.4479649066925049\n","38\n","epoch no.38 train no.1  loss = 0.38932985067367554\n","epoch no.38 train no.11  loss = 0.404592365026474\n","epoch no.38 train no.21  loss = 0.3486593961715698\n","epoch no.38 train no.31  loss = 0.39221325516700745\n","39\n","epoch no.39 train no.1  loss = 0.3434325158596039\n","epoch no.39 train no.11  loss = 0.34878939390182495\n","epoch no.39 train no.21  loss = 0.30354878306388855\n","epoch no.39 train no.31  loss = 0.35090696811676025\n","40\n","epoch no.40 train no.1  loss = 0.2948780059814453\n","epoch no.40 train no.11  loss = 0.29485154151916504\n","epoch no.40 train no.21  loss = 0.25581875443458557\n","epoch no.40 train no.31  loss = 0.2935793101787567\n","41\n","epoch no.41 train no.1  loss = 0.2737770080566406\n","epoch no.41 train no.11  loss = 0.27941766381263733\n","epoch no.41 train no.21  loss = 0.2330303192138672\n","epoch no.41 train no.31  loss = 0.26147884130477905\n","42\n","epoch no.42 train no.1  loss = 0.2321540266275406\n","epoch no.42 train no.11  loss = 0.2332705855369568\n","epoch no.42 train no.21  loss = 0.20385004580020905\n","epoch no.42 train no.31  loss = 0.2385944128036499\n","43\n","epoch no.43 train no.1  loss = 0.20178721845149994\n","epoch no.43 train no.11  loss = 0.2024611532688141\n","epoch no.43 train no.21  loss = 0.18965579569339752\n","epoch no.43 train no.31  loss = 0.20870386064052582\n","44\n","epoch no.44 train no.1  loss = 0.17836956679821014\n","epoch no.44 train no.11  loss = 0.1734410524368286\n","epoch no.44 train no.21  loss = 0.1622406542301178\n","epoch no.44 train no.31  loss = 0.18908344209194183\n","45\n","epoch no.45 train no.1  loss = 0.164617657661438\n","epoch no.45 train no.11  loss = 0.16170357167720795\n","epoch no.45 train no.21  loss = 0.14509184658527374\n","epoch no.45 train no.31  loss = 0.166477233171463\n","46\n","epoch no.46 train no.1  loss = 0.14449703693389893\n","epoch no.46 train no.11  loss = 0.1413475126028061\n","epoch no.46 train no.21  loss = 0.1266259253025055\n","epoch no.46 train no.31  loss = 0.14087963104248047\n","47\n","epoch no.47 train no.1  loss = 0.12934599816799164\n","epoch no.47 train no.11  loss = 0.1318557858467102\n","epoch no.47 train no.21  loss = 0.11906253546476364\n","epoch no.47 train no.31  loss = 0.12647828459739685\n","48\n","epoch no.48 train no.1  loss = 0.11268948018550873\n","epoch no.48 train no.11  loss = 0.11814434826374054\n","epoch no.48 train no.21  loss = 0.11121869087219238\n","epoch no.48 train no.31  loss = 0.12252742797136307\n","49\n","epoch no.49 train no.1  loss = 0.11017820984125137\n","epoch no.49 train no.11  loss = 0.10246600210666656\n","epoch no.49 train no.21  loss = 0.09504100680351257\n","epoch no.49 train no.31  loss = 0.10872068256139755\n","50\n","epoch no.50 train no.1  loss = 0.10280011594295502\n","epoch no.50 train no.11  loss = 0.09350527822971344\n","epoch no.50 train no.21  loss = 0.08692251890897751\n","epoch no.50 train no.31  loss = 0.09352611750364304\n","51\n","epoch no.51 train no.1  loss = 0.08655059337615967\n","epoch no.51 train no.11  loss = 0.09106422960758209\n","epoch no.51 train no.21  loss = 0.08147305995225906\n","epoch no.51 train no.31  loss = 0.08779352903366089\n","52\n","epoch no.52 train no.1  loss = 0.08458228409290314\n","epoch no.52 train no.11  loss = 0.08729667216539383\n","epoch no.52 train no.21  loss = 0.07281185686588287\n","epoch no.52 train no.31  loss = 0.08731408417224884\n","53\n","epoch no.53 train no.1  loss = 0.07804315537214279\n","epoch no.53 train no.11  loss = 0.07438858598470688\n","epoch no.53 train no.21  loss = 0.07181379944086075\n","epoch no.53 train no.31  loss = 0.08127451688051224\n","54\n","epoch no.54 train no.1  loss = 0.08109166473150253\n","epoch no.54 train no.11  loss = 0.07247767597436905\n","epoch no.54 train no.21  loss = 0.06540421396493912\n","epoch no.54 train no.31  loss = 0.0717092752456665\n","55\n","epoch no.55 train no.1  loss = 0.07583924382925034\n","epoch no.55 train no.11  loss = 0.07595956325531006\n","epoch no.55 train no.21  loss = 0.07112704962491989\n","epoch no.55 train no.31  loss = 0.07415377348661423\n","56\n","epoch no.56 train no.1  loss = 0.06296023726463318\n","epoch no.56 train no.11  loss = 0.06392752379179001\n","epoch no.56 train no.21  loss = 0.06935398280620575\n","epoch no.56 train no.31  loss = 0.0660238116979599\n","57\n","epoch no.57 train no.1  loss = 0.05940279737114906\n","epoch no.57 train no.11  loss = 0.058540962636470795\n","epoch no.57 train no.21  loss = 0.05911098048090935\n","epoch no.57 train no.31  loss = 0.06262926012277603\n","58\n","epoch no.58 train no.1  loss = 0.059251222759485245\n","epoch no.58 train no.11  loss = 0.05898164585232735\n","epoch no.58 train no.21  loss = 0.05454270914196968\n","epoch no.58 train no.31  loss = 0.05912834033370018\n","59\n","epoch no.59 train no.1  loss = 0.05364379659295082\n","epoch no.59 train no.11  loss = 0.05292307585477829\n","epoch no.59 train no.21  loss = 0.051979679614305496\n","epoch no.59 train no.31  loss = 0.05669347941875458\n","60\n","epoch no.60 train no.1  loss = 0.04787880927324295\n","epoch no.60 train no.11  loss = 0.04971429705619812\n","epoch no.60 train no.21  loss = 0.0454714372754097\n","epoch no.60 train no.31  loss = 0.05195915699005127\n","61\n","epoch no.61 train no.1  loss = 0.04621133580803871\n","epoch no.61 train no.11  loss = 0.04667951911687851\n","epoch no.61 train no.21  loss = 0.04391501098871231\n","epoch no.61 train no.31  loss = 0.04799804836511612\n","62\n","epoch no.62 train no.1  loss = 0.04404585808515549\n","epoch no.62 train no.11  loss = 0.044252198189496994\n","epoch no.62 train no.21  loss = 0.04299025982618332\n","epoch no.62 train no.31  loss = 0.044528305530548096\n","63\n","epoch no.63 train no.1  loss = 0.043059833347797394\n","epoch no.63 train no.11  loss = 0.04482995346188545\n","epoch no.63 train no.21  loss = 0.04420473426580429\n","epoch no.63 train no.31  loss = 0.04320206865668297\n","64\n","epoch no.64 train no.1  loss = 0.04142205789685249\n","epoch no.64 train no.11  loss = 0.037063002586364746\n","epoch no.64 train no.21  loss = 0.04111539572477341\n","epoch no.64 train no.31  loss = 0.04404372721910477\n","65\n","epoch no.65 train no.1  loss = 0.04180210083723068\n","epoch no.65 train no.11  loss = 0.03996982425451279\n","epoch no.65 train no.21  loss = 0.04002132639288902\n","epoch no.65 train no.31  loss = 0.03974936529994011\n","66\n","epoch no.66 train no.1  loss = 0.040719520300626755\n","epoch no.66 train no.11  loss = 0.038601454347372055\n","epoch no.66 train no.21  loss = 0.03645901381969452\n","epoch no.66 train no.31  loss = 0.04074198380112648\n","67\n","epoch no.67 train no.1  loss = 0.04120554029941559\n","epoch no.67 train no.11  loss = 0.038689788430929184\n","epoch no.67 train no.21  loss = 0.03680359572172165\n","epoch no.67 train no.31  loss = 0.03852415457367897\n","68\n","epoch no.68 train no.1  loss = 0.03789719194173813\n","epoch no.68 train no.11  loss = 0.03621303290128708\n","epoch no.68 train no.21  loss = 0.03459543734788895\n","epoch no.68 train no.31  loss = 0.037447378039360046\n","69\n","epoch no.69 train no.1  loss = 0.03504389151930809\n","epoch no.69 train no.11  loss = 0.03584738075733185\n","epoch no.69 train no.21  loss = 0.03778929263353348\n","epoch no.69 train no.31  loss = 0.03450239449739456\n","70\n","epoch no.70 train no.1  loss = 0.03928837925195694\n","epoch no.70 train no.11  loss = 0.03218232840299606\n","epoch no.70 train no.21  loss = 0.03856102004647255\n","epoch no.70 train no.31  loss = 0.037229593843221664\n","71\n","epoch no.71 train no.1  loss = 0.044863950461149216\n","epoch no.71 train no.11  loss = 0.03560531139373779\n","epoch no.71 train no.21  loss = 0.03578559681773186\n","epoch no.71 train no.31  loss = 0.035046108067035675\n","72\n","epoch no.72 train no.1  loss = 0.04021254554390907\n","epoch no.72 train no.11  loss = 0.0339055135846138\n","epoch no.72 train no.21  loss = 0.036591269075870514\n","epoch no.72 train no.31  loss = 0.0351652093231678\n","73\n","epoch no.73 train no.1  loss = 0.032020535320043564\n","epoch no.73 train no.11  loss = 0.03240561485290527\n","epoch no.73 train no.21  loss = 0.031729355454444885\n","epoch no.73 train no.31  loss = 0.03574420511722565\n","74\n","epoch no.74 train no.1  loss = 0.02959379181265831\n","epoch no.74 train no.11  loss = 0.033200547099113464\n","epoch no.74 train no.21  loss = 0.03417390584945679\n","epoch no.74 train no.31  loss = 0.033256810158491135\n","75\n","epoch no.75 train no.1  loss = 0.03245284780859947\n","epoch no.75 train no.11  loss = 0.0329308845102787\n","epoch no.75 train no.21  loss = 0.030357953161001205\n","epoch no.75 train no.31  loss = 0.0317947082221508\n","76\n","epoch no.76 train no.1  loss = 0.030040299519896507\n","epoch no.76 train no.11  loss = 0.03431973233819008\n","epoch no.76 train no.21  loss = 0.032236501574516296\n","epoch no.76 train no.31  loss = 0.033018164336681366\n","77\n","epoch no.77 train no.1  loss = 0.02805521711707115\n","epoch no.77 train no.11  loss = 0.03309343755245209\n","epoch no.77 train no.21  loss = 0.031562503427267075\n","epoch no.77 train no.31  loss = 0.03184555843472481\n","78\n","epoch no.78 train no.1  loss = 0.03349463269114494\n","epoch no.78 train no.11  loss = 0.030697399750351906\n","epoch no.78 train no.21  loss = 0.03128277510404587\n","epoch no.78 train no.31  loss = 0.03105645626783371\n","79\n","epoch no.79 train no.1  loss = 0.030569685623049736\n","epoch no.79 train no.11  loss = 0.03035980463027954\n","epoch no.79 train no.21  loss = 0.031314630061388016\n","epoch no.79 train no.31  loss = 0.02915043570101261\n","80\n","epoch no.80 train no.1  loss = 0.028662508353590965\n","epoch no.80 train no.11  loss = 0.02886606752872467\n","epoch no.80 train no.21  loss = 0.026279136538505554\n","epoch no.80 train no.31  loss = 0.03114752098917961\n","81\n","epoch no.81 train no.1  loss = 0.03725224733352661\n","epoch no.81 train no.11  loss = 0.027510765939950943\n","epoch no.81 train no.21  loss = 0.024867253378033638\n","epoch no.81 train no.31  loss = 0.027428962290287018\n","82\n","epoch no.82 train no.1  loss = 0.032663363963365555\n","epoch no.82 train no.11  loss = 0.026073254644870758\n","epoch no.82 train no.21  loss = 0.026185210794210434\n","epoch no.82 train no.31  loss = 0.028291989117860794\n","83\n","epoch no.83 train no.1  loss = 0.029381245374679565\n","epoch no.83 train no.11  loss = 0.03324523940682411\n","epoch no.83 train no.21  loss = 0.03377881273627281\n","epoch no.83 train no.31  loss = 0.03271416574716568\n","84\n","epoch no.84 train no.1  loss = 0.030809743329882622\n","epoch no.84 train no.11  loss = 0.039859939366579056\n","epoch no.84 train no.21  loss = 0.029673928394913673\n","epoch no.84 train no.31  loss = 0.03751823306083679\n","85\n","epoch no.85 train no.1  loss = 0.030373740941286087\n","epoch no.85 train no.11  loss = 0.033836979418992996\n","epoch no.85 train no.21  loss = 0.03028525598347187\n","epoch no.85 train no.31  loss = 0.030782999470829964\n","86\n","epoch no.86 train no.1  loss = 0.030712509527802467\n","epoch no.86 train no.11  loss = 0.029632428660988808\n","epoch no.86 train no.21  loss = 0.03412698209285736\n","epoch no.86 train no.31  loss = 0.0346071757376194\n","87\n","epoch no.87 train no.1  loss = 0.03146611526608467\n","epoch no.87 train no.11  loss = 0.02920648828148842\n","epoch no.87 train no.21  loss = 0.03489629551768303\n","epoch no.87 train no.31  loss = 0.02886156737804413\n","88\n","epoch no.88 train no.1  loss = 0.03335469216108322\n","epoch no.88 train no.11  loss = 0.03296592831611633\n","epoch no.88 train no.21  loss = 0.034599438309669495\n","epoch no.88 train no.31  loss = 0.03168804943561554\n","89\n","epoch no.89 train no.1  loss = 0.03236826881766319\n","epoch no.89 train no.11  loss = 0.03300514072179794\n","epoch no.89 train no.21  loss = 0.029788406565785408\n","epoch no.89 train no.31  loss = 0.032191913574934006\n","90\n","epoch no.90 train no.1  loss = 0.024865806102752686\n","epoch no.90 train no.11  loss = 0.03131499141454697\n","epoch no.90 train no.21  loss = 0.02615070901811123\n","epoch no.90 train no.31  loss = 0.03498372435569763\n","91\n","epoch no.91 train no.1  loss = 0.025835217908024788\n","epoch no.91 train no.11  loss = 0.029561595991253853\n","epoch no.91 train no.21  loss = 0.02723206765949726\n","epoch no.91 train no.31  loss = 0.02786623127758503\n","92\n","epoch no.92 train no.1  loss = 0.027103809639811516\n","epoch no.92 train no.11  loss = 0.029335029423236847\n","epoch no.92 train no.21  loss = 0.02898118831217289\n","epoch no.92 train no.31  loss = 0.027277415618300438\n","93\n","epoch no.93 train no.1  loss = 0.033831674605607986\n","epoch no.93 train no.11  loss = 0.038811784237623215\n","epoch no.93 train no.21  loss = 0.03264441341161728\n","epoch no.93 train no.31  loss = 0.02630668319761753\n","94\n","epoch no.94 train no.1  loss = 0.029169103130698204\n","epoch no.94 train no.11  loss = 0.032416023313999176\n","epoch no.94 train no.21  loss = 0.02478341944515705\n","epoch no.94 train no.31  loss = 0.025039806962013245\n","95\n","epoch no.95 train no.1  loss = 0.025923313573002815\n","epoch no.95 train no.11  loss = 0.02561509795486927\n","epoch no.95 train no.21  loss = 0.023338669911026955\n","epoch no.95 train no.31  loss = 0.023140547797083855\n","96\n","epoch no.96 train no.1  loss = 0.026641232892870903\n","epoch no.96 train no.11  loss = 0.027975568547844887\n","epoch no.96 train no.21  loss = 0.024355826899409294\n","epoch no.96 train no.31  loss = 0.02662273682653904\n","97\n","epoch no.97 train no.1  loss = 0.027208561077713966\n","epoch no.97 train no.11  loss = 0.026360217481851578\n","epoch no.97 train no.21  loss = 0.025412993505597115\n","epoch no.97 train no.31  loss = 0.029506437480449677\n","98\n","epoch no.98 train no.1  loss = 0.0249685849994421\n","epoch no.98 train no.11  loss = 0.025050833821296692\n","epoch no.98 train no.21  loss = 0.023382507264614105\n","epoch no.98 train no.31  loss = 0.032215941697359085\n","99\n","epoch no.99 train no.1  loss = 0.022176574915647507\n","epoch no.99 train no.11  loss = 0.02922845259308815\n","epoch no.99 train no.21  loss = 0.02766515128314495\n","epoch no.99 train no.31  loss = 0.02627991884946823\n","100\n","epoch no.100 train no.1  loss = 0.02654687874019146\n","epoch no.100 train no.11  loss = 0.028464425355196\n","epoch no.100 train no.21  loss = 0.027898350730538368\n","epoch no.100 train no.31  loss = 0.025947753340005875\n","101\n","epoch no.101 train no.1  loss = 0.025152547284960747\n","epoch no.101 train no.11  loss = 0.025741780176758766\n","epoch no.101 train no.21  loss = 0.023799216374754906\n","epoch no.101 train no.31  loss = 0.026222722604870796\n","102\n","epoch no.102 train no.1  loss = 0.024301819503307343\n","epoch no.102 train no.11  loss = 0.022905664518475533\n","epoch no.102 train no.21  loss = 0.024598613381385803\n","epoch no.102 train no.31  loss = 0.026526814326643944\n","103\n","epoch no.103 train no.1  loss = 0.03058014251291752\n","epoch no.103 train no.11  loss = 0.024513771757483482\n","epoch no.103 train no.21  loss = 0.025140875950455666\n","epoch no.103 train no.31  loss = 0.02685692347586155\n","104\n","epoch no.104 train no.1  loss = 0.04202718660235405\n","epoch no.104 train no.11  loss = 0.026851171627640724\n","epoch no.104 train no.21  loss = 0.027307402342557907\n","epoch no.104 train no.31  loss = 0.027377361431717873\n","105\n","epoch no.105 train no.1  loss = 0.032598067075014114\n","epoch no.105 train no.11  loss = 0.028205424547195435\n","epoch no.105 train no.21  loss = 0.025454562157392502\n","epoch no.105 train no.31  loss = 0.030836081132292747\n","106\n","epoch no.106 train no.1  loss = 0.029265612363815308\n","epoch no.106 train no.11  loss = 0.031135518103837967\n","epoch no.106 train no.21  loss = 0.029401548206806183\n","epoch no.106 train no.31  loss = 0.03168410435318947\n","107\n","epoch no.107 train no.1  loss = 0.030392231419682503\n","epoch no.107 train no.11  loss = 0.03305872157216072\n","epoch no.107 train no.21  loss = 0.03655482456088066\n","epoch no.107 train no.31  loss = 0.033464714884757996\n","108\n","epoch no.108 train no.1  loss = 0.03797798603773117\n","epoch no.108 train no.11  loss = 0.03869776800274849\n","epoch no.108 train no.21  loss = 0.05790996551513672\n","epoch no.108 train no.31  loss = 0.03030472621321678\n","109\n","epoch no.109 train no.1  loss = 0.036790382117033005\n","epoch no.109 train no.11  loss = 0.03475065901875496\n","epoch no.109 train no.21  loss = 0.04029792547225952\n","epoch no.109 train no.31  loss = 0.030936555936932564\n","110\n","epoch no.110 train no.1  loss = 0.04903436824679375\n","epoch no.110 train no.11  loss = 0.03327696770429611\n","epoch no.110 train no.21  loss = 0.03383568674325943\n","epoch no.110 train no.31  loss = 0.03422548249363899\n","111\n","epoch no.111 train no.1  loss = 0.04507274925708771\n","epoch no.111 train no.11  loss = 0.046801719814538956\n","epoch no.111 train no.21  loss = 0.032360102981328964\n","epoch no.111 train no.31  loss = 0.034797243773937225\n","112\n","epoch no.112 train no.1  loss = 0.043155916035175323\n","epoch no.112 train no.11  loss = 0.0346049964427948\n","epoch no.112 train no.21  loss = 0.038259007036685944\n","epoch no.112 train no.31  loss = 0.027713650837540627\n","113\n","epoch no.113 train no.1  loss = 0.03516126051545143\n","epoch no.113 train no.11  loss = 0.029537688940763474\n","epoch no.113 train no.21  loss = 0.030221130698919296\n","epoch no.113 train no.31  loss = 0.027983715757727623\n","114\n","epoch no.114 train no.1  loss = 0.029929907992482185\n","epoch no.114 train no.11  loss = 0.029143977910280228\n","epoch no.114 train no.21  loss = 0.025022171437740326\n","epoch no.114 train no.31  loss = 0.021884510293602943\n","115\n","epoch no.115 train no.1  loss = 0.029879605397582054\n","epoch no.115 train no.11  loss = 0.02532193623483181\n","epoch no.115 train no.21  loss = 0.02832341007888317\n","epoch no.115 train no.31  loss = 0.021240826696157455\n","116\n","epoch no.116 train no.1  loss = 0.027271810919046402\n","epoch no.116 train no.11  loss = 0.023323774337768555\n","epoch no.116 train no.21  loss = 0.02383723296225071\n","epoch no.116 train no.31  loss = 0.022679243236780167\n","117\n","epoch no.117 train no.1  loss = 0.027271809056401253\n","epoch no.117 train no.11  loss = 0.03402499109506607\n","epoch no.117 train no.21  loss = 0.02415001019835472\n","epoch no.117 train no.31  loss = 0.023239025846123695\n","118\n","epoch no.118 train no.1  loss = 0.03489359840750694\n","epoch no.118 train no.11  loss = 0.031408973038196564\n","epoch no.118 train no.21  loss = 0.02051144652068615\n","epoch no.118 train no.31  loss = 0.019587503746151924\n","119\n","epoch no.119 train no.1  loss = 0.03929807245731354\n","epoch no.119 train no.11  loss = 0.026719441637396812\n","epoch no.119 train no.21  loss = 0.023956531658768654\n","epoch no.119 train no.31  loss = 0.01924164406955242\n","120\n","epoch no.120 train no.1  loss = 0.02218015491962433\n","epoch no.120 train no.11  loss = 0.028898339718580246\n","epoch no.120 train no.21  loss = 0.027371155098080635\n","epoch no.120 train no.31  loss = 0.020482901483774185\n","121\n","epoch no.121 train no.1  loss = 0.02264573611319065\n","epoch no.121 train no.11  loss = 0.01990862563252449\n","epoch no.121 train no.21  loss = 0.022119248285889626\n","epoch no.121 train no.31  loss = 0.01916927471756935\n","122\n","epoch no.122 train no.1  loss = 0.026057299226522446\n","epoch no.122 train no.11  loss = 0.020932963117957115\n","epoch no.122 train no.21  loss = 0.017251061275601387\n","epoch no.122 train no.31  loss = 0.020743470638990402\n","123\n","epoch no.123 train no.1  loss = 0.020497549325227737\n","epoch no.123 train no.11  loss = 0.021245447918772697\n","epoch no.123 train no.21  loss = 0.020687611773610115\n","epoch no.123 train no.31  loss = 0.04275735840201378\n","124\n","epoch no.124 train no.1  loss = 0.01906493492424488\n","epoch no.124 train no.11  loss = 0.01917511783540249\n","epoch no.124 train no.21  loss = 0.02043296955525875\n","epoch no.124 train no.31  loss = 0.031511034816503525\n","125\n","epoch no.125 train no.1  loss = 0.020006807520985603\n","epoch no.125 train no.11  loss = 0.02018069662153721\n","epoch no.125 train no.21  loss = 0.023647084832191467\n","epoch no.125 train no.31  loss = 0.023209840059280396\n","126\n","epoch no.126 train no.1  loss = 0.020344499498605728\n","epoch no.126 train no.11  loss = 0.020916320383548737\n","epoch no.126 train no.21  loss = 0.02277306839823723\n","epoch no.126 train no.31  loss = 0.019478991627693176\n","127\n","epoch no.127 train no.1  loss = 0.027341581881046295\n","epoch no.127 train no.11  loss = 0.027014220133423805\n","epoch no.127 train no.21  loss = 0.02231936901807785\n","epoch no.127 train no.31  loss = 0.017922960221767426\n","128\n","epoch no.128 train no.1  loss = 0.021821973845362663\n","epoch no.128 train no.11  loss = 0.021880464628338814\n","epoch no.128 train no.21  loss = 0.02136944979429245\n","epoch no.128 train no.31  loss = 0.020620357245206833\n","129\n","epoch no.129 train no.1  loss = 0.024969354271888733\n","epoch no.129 train no.11  loss = 0.022162389010190964\n","epoch no.129 train no.21  loss = 0.02175205759704113\n","epoch no.129 train no.31  loss = 0.024902351200580597\n","130\n","epoch no.130 train no.1  loss = 0.020237522199749947\n","epoch no.130 train no.11  loss = 0.018718594685196877\n","epoch no.130 train no.21  loss = 0.019699541851878166\n","epoch no.130 train no.31  loss = 0.021474460139870644\n","131\n","epoch no.131 train no.1  loss = 0.019140642136335373\n","epoch no.131 train no.11  loss = 0.02175031416118145\n","epoch no.131 train no.21  loss = 0.018088651821017265\n","epoch no.131 train no.31  loss = 0.021952630952000618\n","132\n","epoch no.132 train no.1  loss = 0.018113046884536743\n","epoch no.132 train no.11  loss = 0.018187034875154495\n","epoch no.132 train no.21  loss = 0.027351265773177147\n","epoch no.132 train no.31  loss = 0.021868383511900902\n","133\n","epoch no.133 train no.1  loss = 0.019805166870355606\n","epoch no.133 train no.11  loss = 0.020625730976462364\n","epoch no.133 train no.21  loss = 0.025505011901259422\n","epoch no.133 train no.31  loss = 0.019311189651489258\n","134\n","epoch no.134 train no.1  loss = 0.023107651621103287\n","epoch no.134 train no.11  loss = 0.02152758650481701\n","epoch no.134 train no.21  loss = 0.022461822256445885\n","epoch no.134 train no.31  loss = 0.01930052600800991\n","135\n","epoch no.135 train no.1  loss = 0.01931844837963581\n","epoch no.135 train no.11  loss = 0.024721909314393997\n","epoch no.135 train no.21  loss = 0.02002722956240177\n","epoch no.135 train no.31  loss = 0.017021911218762398\n","136\n","epoch no.136 train no.1  loss = 0.015856698155403137\n","epoch no.136 train no.11  loss = 0.01938575878739357\n","epoch no.136 train no.21  loss = 0.019261913374066353\n","epoch no.136 train no.31  loss = 0.019392574205994606\n","137\n","epoch no.137 train no.1  loss = 0.020489610731601715\n","epoch no.137 train no.11  loss = 0.016793683171272278\n","epoch no.137 train no.21  loss = 0.017404163256287575\n","epoch no.137 train no.31  loss = 0.021518317982554436\n","138\n","epoch no.138 train no.1  loss = 0.019609659910202026\n","epoch no.138 train no.11  loss = 0.022609392181038857\n","epoch no.138 train no.21  loss = 0.01860171929001808\n","epoch no.138 train no.31  loss = 0.01746818982064724\n","139\n","epoch no.139 train no.1  loss = 0.02249995619058609\n","epoch no.139 train no.11  loss = 0.02467287704348564\n","epoch no.139 train no.21  loss = 0.022385483607649803\n","epoch no.139 train no.31  loss = 0.025312069803476334\n","140\n","epoch no.140 train no.1  loss = 0.028613392263650894\n","epoch no.140 train no.11  loss = 0.031234098598361015\n","epoch no.140 train no.21  loss = 0.02625124715268612\n","epoch no.140 train no.31  loss = 0.02515139989554882\n","141\n","epoch no.141 train no.1  loss = 0.02264534868299961\n","epoch no.141 train no.11  loss = 0.021571511402726173\n","epoch no.141 train no.21  loss = 0.024212703108787537\n","epoch no.141 train no.31  loss = 0.024122092872858047\n","142\n","epoch no.142 train no.1  loss = 0.031514864414930344\n","epoch no.142 train no.11  loss = 0.023293545469641685\n","epoch no.142 train no.21  loss = 0.017814157530665398\n","epoch no.142 train no.31  loss = 0.022953657433390617\n","143\n","epoch no.143 train no.1  loss = 0.0270894356071949\n","epoch no.143 train no.11  loss = 0.027802003547549248\n","epoch no.143 train no.21  loss = 0.023196568712592125\n","epoch no.143 train no.31  loss = 0.027859391644597054\n","144\n","epoch no.144 train no.1  loss = 0.02585262432694435\n","epoch no.144 train no.11  loss = 0.02147931046783924\n","epoch no.144 train no.21  loss = 0.023321524262428284\n","epoch no.144 train no.31  loss = 0.02286946401000023\n","145\n","epoch no.145 train no.1  loss = 0.022735031321644783\n","epoch no.145 train no.11  loss = 0.026500597596168518\n","epoch no.145 train no.21  loss = 0.02769617922604084\n","epoch no.145 train no.31  loss = 0.02227165363729\n","146\n","epoch no.146 train no.1  loss = 0.023347772657871246\n","epoch no.146 train no.11  loss = 0.0252077579498291\n","epoch no.146 train no.21  loss = 0.026226941496133804\n","epoch no.146 train no.31  loss = 0.024750063195824623\n","147\n","epoch no.147 train no.1  loss = 0.019188107922673225\n","epoch no.147 train no.11  loss = 0.027451327070593834\n","epoch no.147 train no.21  loss = 0.023071760311722755\n","epoch no.147 train no.31  loss = 0.017629629001021385\n","148\n","epoch no.148 train no.1  loss = 0.01655246689915657\n","epoch no.148 train no.11  loss = 0.025291383266448975\n","epoch no.148 train no.21  loss = 0.02249222993850708\n","epoch no.148 train no.31  loss = 0.019817864522337914\n","149\n","epoch no.149 train no.1  loss = 0.016779659315943718\n","epoch no.149 train no.11  loss = 0.021149521693587303\n","epoch no.149 train no.21  loss = 0.027079883962869644\n","epoch no.149 train no.31  loss = 0.020150652155280113\n","150\n","epoch no.150 train no.1  loss = 0.01639520190656185\n","epoch no.150 train no.11  loss = 0.021823367103934288\n","epoch no.150 train no.21  loss = 0.02135464735329151\n","epoch no.150 train no.31  loss = 0.021441813558340073\n","151\n","epoch no.151 train no.1  loss = 0.014538153074681759\n","epoch no.151 train no.11  loss = 0.0171920508146286\n","epoch no.151 train no.21  loss = 0.025465311482548714\n","epoch no.151 train no.31  loss = 0.036968134343624115\n","152\n","epoch no.152 train no.1  loss = 0.017846500501036644\n","epoch no.152 train no.11  loss = 0.03571183979511261\n","epoch no.152 train no.21  loss = 0.022689294070005417\n","epoch no.152 train no.31  loss = 0.03526683151721954\n","153\n","epoch no.153 train no.1  loss = 0.017048759385943413\n","epoch no.153 train no.11  loss = 0.03843216598033905\n","epoch no.153 train no.21  loss = 0.019329745322465897\n","epoch no.153 train no.31  loss = 0.03204580768942833\n","154\n","epoch no.154 train no.1  loss = 0.021770482882857323\n","epoch no.154 train no.11  loss = 0.021020522341132164\n","epoch no.154 train no.21  loss = 0.021406201645731926\n","epoch no.154 train no.31  loss = 0.026212768629193306\n","155\n","epoch no.155 train no.1  loss = 0.01490969117730856\n","epoch no.155 train no.11  loss = 0.01954459957778454\n","epoch no.155 train no.21  loss = 0.02422513999044895\n","epoch no.155 train no.31  loss = 0.06132079288363457\n","156\n","epoch no.156 train no.1  loss = 0.018090613186359406\n","epoch no.156 train no.11  loss = 0.018068397417664528\n","epoch no.156 train no.21  loss = 0.018778063356876373\n","epoch no.156 train no.31  loss = 0.05496707186102867\n","157\n","epoch no.157 train no.1  loss = 0.01543775200843811\n","epoch no.157 train no.11  loss = 0.01430890616029501\n","epoch no.157 train no.21  loss = 0.017385266721248627\n","epoch no.157 train no.31  loss = 0.059813935309648514\n","158\n","epoch no.158 train no.1  loss = 0.021411985158920288\n","epoch no.158 train no.11  loss = 0.019374359399080276\n","epoch no.158 train no.21  loss = 0.018124938011169434\n","epoch no.158 train no.31  loss = 0.07068522274494171\n","159\n","epoch no.159 train no.1  loss = 0.02061636745929718\n","epoch no.159 train no.11  loss = 0.027964551001787186\n","epoch no.159 train no.21  loss = 0.018307028338313103\n","epoch no.159 train no.31  loss = 0.025890031829476357\n","160\n","epoch no.160 train no.1  loss = 0.027445172891020775\n","epoch no.160 train no.11  loss = 0.026920361444354057\n","epoch no.160 train no.21  loss = 0.032346516847610474\n","epoch no.160 train no.31  loss = 0.025541890412569046\n","161\n","epoch no.161 train no.1  loss = 0.02310323715209961\n","epoch no.161 train no.11  loss = 0.023298203945159912\n","epoch no.161 train no.21  loss = 0.020616434514522552\n","epoch no.161 train no.31  loss = 0.02210567146539688\n","162\n","epoch no.162 train no.1  loss = 0.017383970320224762\n","epoch no.162 train no.11  loss = 0.017900872975587845\n","epoch no.162 train no.21  loss = 0.013748281635344028\n","epoch no.162 train no.31  loss = 0.017909560352563858\n","163\n","epoch no.163 train no.1  loss = 0.01388061884790659\n","epoch no.163 train no.11  loss = 0.014766509644687176\n","epoch no.163 train no.21  loss = 0.017827270552515984\n","epoch no.163 train no.31  loss = 0.020837554708123207\n","164\n","epoch no.164 train no.1  loss = 0.013640489429235458\n","epoch no.164 train no.11  loss = 0.02070983126759529\n","epoch no.164 train no.21  loss = 0.01706385612487793\n","epoch no.164 train no.31  loss = 0.013757049106061459\n","165\n","epoch no.165 train no.1  loss = 0.013165595009922981\n","epoch no.165 train no.11  loss = 0.015772292390465736\n","epoch no.165 train no.21  loss = 0.017221353948116302\n","epoch no.165 train no.31  loss = 0.014164888300001621\n","166\n","epoch no.166 train no.1  loss = 0.013243025168776512\n","epoch no.166 train no.11  loss = 0.019543539732694626\n","epoch no.166 train no.21  loss = 0.014870015904307365\n","epoch no.166 train no.31  loss = 0.01841387338936329\n","167\n","epoch no.167 train no.1  loss = 0.014444764703512192\n","epoch no.167 train no.11  loss = 0.016783861443400383\n","epoch no.167 train no.21  loss = 0.019720599055290222\n","epoch no.167 train no.31  loss = 0.017873816192150116\n","168\n","epoch no.168 train no.1  loss = 0.020151188597083092\n","epoch no.168 train no.11  loss = 0.017715999856591225\n","epoch no.168 train no.21  loss = 0.012636912986636162\n","epoch no.168 train no.31  loss = 0.017500994727015495\n","169\n","epoch no.169 train no.1  loss = 0.012693519704043865\n","epoch no.169 train no.11  loss = 0.020172836259007454\n","epoch no.169 train no.21  loss = 0.021548297256231308\n","epoch no.169 train no.31  loss = 0.024000443518161774\n","170\n","epoch no.170 train no.1  loss = 0.014111435040831566\n","epoch no.170 train no.11  loss = 0.023619893938302994\n","epoch no.170 train no.21  loss = 0.01549537479877472\n","epoch no.170 train no.31  loss = 0.01878771372139454\n","171\n","epoch no.171 train no.1  loss = 0.01798086054623127\n","epoch no.171 train no.11  loss = 0.019229406490921974\n","epoch no.171 train no.21  loss = 0.016980834305286407\n","epoch no.171 train no.31  loss = 0.02115597203373909\n","172\n","epoch no.172 train no.1  loss = 0.019460201263427734\n","epoch no.172 train no.11  loss = 0.027700010687112808\n","epoch no.172 train no.21  loss = 0.02573419362306595\n","epoch no.172 train no.31  loss = 0.018878992646932602\n","173\n","epoch no.173 train no.1  loss = 0.028634602203965187\n","epoch no.173 train no.11  loss = 0.023229844868183136\n","epoch no.173 train no.21  loss = 0.019458305090665817\n","epoch no.173 train no.31  loss = 0.022114228457212448\n","174\n","epoch no.174 train no.1  loss = 0.021181484684348106\n","epoch no.174 train no.11  loss = 0.03423590585589409\n","epoch no.174 train no.21  loss = 0.01977161504328251\n","epoch no.174 train no.31  loss = 0.020074406638741493\n","175\n","epoch no.175 train no.1  loss = 0.021703332662582397\n","epoch no.175 train no.11  loss = 0.02915668487548828\n","epoch no.175 train no.21  loss = 0.02417232096195221\n","epoch no.175 train no.31  loss = 0.01827819272875786\n","176\n","epoch no.176 train no.1  loss = 0.02089964598417282\n","epoch no.176 train no.11  loss = 0.019022932276129723\n","epoch no.176 train no.21  loss = 0.025522686541080475\n","epoch no.176 train no.31  loss = 0.021919943392276764\n","177\n","epoch no.177 train no.1  loss = 0.02756352536380291\n","epoch no.177 train no.11  loss = 0.02031981758773327\n","epoch no.177 train no.21  loss = 0.022744450718164444\n","epoch no.177 train no.31  loss = 0.02542641945183277\n","178\n","epoch no.178 train no.1  loss = 0.019275082275271416\n","epoch no.178 train no.11  loss = 0.02680756337940693\n","epoch no.178 train no.21  loss = 0.02726971171796322\n","epoch no.178 train no.31  loss = 0.02653692476451397\n","179\n","epoch no.179 train no.1  loss = 0.019435973837971687\n","epoch no.179 train no.11  loss = 0.021465474739670753\n","epoch no.179 train no.21  loss = 0.026936938986182213\n","epoch no.179 train no.31  loss = 0.02222772128880024\n","180\n","epoch no.180 train no.1  loss = 0.022281963378190994\n","epoch no.180 train no.11  loss = 0.01767035946249962\n","epoch no.180 train no.21  loss = 0.020734157413244247\n","epoch no.180 train no.31  loss = 0.02805459126830101\n","181\n","epoch no.181 train no.1  loss = 0.022801687940955162\n","epoch no.181 train no.11  loss = 0.019934162497520447\n","epoch no.181 train no.21  loss = 0.017331155017018318\n","epoch no.181 train no.31  loss = 0.019889844581484795\n","182\n","epoch no.182 train no.1  loss = 0.01619684509932995\n","epoch no.182 train no.11  loss = 0.014421702362596989\n","epoch no.182 train no.21  loss = 0.031726714223623276\n","epoch no.182 train no.31  loss = 0.017518674954771996\n","183\n","epoch no.183 train no.1  loss = 0.018318424001336098\n","epoch no.183 train no.11  loss = 0.024203909561038017\n","epoch no.183 train no.21  loss = 0.030870888382196426\n","epoch no.183 train no.31  loss = 0.023710401728749275\n","184\n","epoch no.184 train no.1  loss = 0.016169141978025436\n","epoch no.184 train no.11  loss = 0.0221813153475523\n","epoch no.184 train no.21  loss = 0.022208962589502335\n","epoch no.184 train no.31  loss = 0.01651274785399437\n","185\n","epoch no.185 train no.1  loss = 0.016680454835295677\n","epoch no.185 train no.11  loss = 0.02637503482401371\n","epoch no.185 train no.21  loss = 0.020378658547997475\n","epoch no.185 train no.31  loss = 0.017071614041924477\n","186\n","epoch no.186 train no.1  loss = 0.017206180840730667\n","epoch no.186 train no.11  loss = 0.028028085827827454\n","epoch no.186 train no.21  loss = 0.019116917625069618\n","epoch no.186 train no.31  loss = 0.013905739411711693\n","187\n","epoch no.187 train no.1  loss = 0.014968838542699814\n","epoch no.187 train no.11  loss = 0.018422115594148636\n","epoch no.187 train no.21  loss = 0.022780949249863625\n","epoch no.187 train no.31  loss = 0.015118040144443512\n","188\n","epoch no.188 train no.1  loss = 0.016295921057462692\n","epoch no.188 train no.11  loss = 0.022202637046575546\n","epoch no.188 train no.21  loss = 0.019559301435947418\n","epoch no.188 train no.31  loss = 0.01518174260854721\n","189\n","epoch no.189 train no.1  loss = 0.015391502529382706\n","epoch no.189 train no.11  loss = 0.018941551446914673\n","epoch no.189 train no.21  loss = 0.01884029060602188\n","epoch no.189 train no.31  loss = 0.014609954319894314\n","190\n","epoch no.190 train no.1  loss = 0.013929056003689766\n","epoch no.190 train no.11  loss = 0.016133246943354607\n","epoch no.190 train no.21  loss = 0.017918549478054047\n","epoch no.190 train no.31  loss = 0.015348250046372414\n","191\n","epoch no.191 train no.1  loss = 0.014126473106443882\n","epoch no.191 train no.11  loss = 0.014776303432881832\n","epoch no.191 train no.21  loss = 0.015958696603775024\n","epoch no.191 train no.31  loss = 0.01705768331885338\n","192\n","epoch no.192 train no.1  loss = 0.01969575695693493\n","epoch no.192 train no.11  loss = 0.016049468889832497\n","epoch no.192 train no.21  loss = 0.015217256732285023\n","epoch no.192 train no.31  loss = 0.025978991761803627\n","193\n","epoch no.193 train no.1  loss = 0.015147495083510876\n","epoch no.193 train no.11  loss = 0.017549073323607445\n","epoch no.193 train no.21  loss = 0.013197473250329494\n","epoch no.193 train no.31  loss = 0.01752670668065548\n","194\n","epoch no.194 train no.1  loss = 0.01628291793167591\n","epoch no.194 train no.11  loss = 0.016819849610328674\n","epoch no.194 train no.21  loss = 0.018092401325702667\n","epoch no.194 train no.31  loss = 0.019762329757213593\n","195\n","epoch no.195 train no.1  loss = 0.015446281991899014\n","epoch no.195 train no.11  loss = 0.013593355193734169\n","epoch no.195 train no.21  loss = 0.018838901072740555\n","epoch no.195 train no.31  loss = 0.018782304599881172\n","196\n","epoch no.196 train no.1  loss = 0.017253253608942032\n","epoch no.196 train no.11  loss = 0.014446931891143322\n","epoch no.196 train no.21  loss = 0.013914459384977818\n","epoch no.196 train no.31  loss = 0.015373407863080502\n","197\n","epoch no.197 train no.1  loss = 0.012074392288923264\n","epoch no.197 train no.11  loss = 0.013208865188062191\n","epoch no.197 train no.21  loss = 0.01373312808573246\n","epoch no.197 train no.31  loss = 0.01440451666712761\n","198\n","epoch no.198 train no.1  loss = 0.011935551650822163\n","epoch no.198 train no.11  loss = 0.017671672627329826\n","epoch no.198 train no.21  loss = 0.01044501643627882\n","epoch no.198 train no.31  loss = 0.012245165184140205\n","199\n","epoch no.199 train no.1  loss = 0.011352204717695713\n","epoch no.199 train no.11  loss = 0.01417975127696991\n","epoch no.199 train no.21  loss = 0.012668343260884285\n","epoch no.199 train no.31  loss = 0.011369491927325726\n","save!\n"]}]},{"cell_type":"code","metadata":{"id":"fQZCV6OoTp0j"},"source":[""],"execution_count":null,"outputs":[]}]}